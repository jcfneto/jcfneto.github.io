---
title: "The curse of AI and the intentional stance"
categories:
  - philosophy
tags:
  - AI
  - intentional stance
use_math: false
published: false
---

---
title: "The curse of AI and the intentional stance"
categories:
  - philosophy
tags:
  - AI
  - intentional stance
use_math: false
published: false
---

"Yes, but that's not real intelligence" is a common retort to any mechanism used to solve a problem that appears to require an intelligent agent to solve. This is the 'curse of AI', as described nicely in the following [cartoon](http://comic.artificial-intelligence.com/ai-comics-0007.pdf). As soon as a mechanistic explanation of how a problem can be solved is provided the program/machine ceases to be seen as intelligent, but as a dumb and mindless machine. These two things are in a way seen as incompatible. 

The curse of AI could be understood by using Dennett's intentional stance. Dennett's ideas about the intentional stance can be summarized as follows. When interacting with things in the world, we can adopt a number of _stances_ towards them. We can treat them as physical objects, parsing them into their physical components, and using laws of physics to understand how those components interact to produce the behavior the object. This is adopting a _physical stance_. 

We can, alternatively, adopt a _design stance_, and understand the object in terms of its functions. We can think about what different components are _for_. This helps us interact and understand certain objects, including, of course, those that are explicitly designed by us. E.g. when we drive a car, we typically adopt a design stance, and think about using the steering wheel for steering the car. We don't tend to focus on understanding the mechanics or physics underlying how the steering works... unless something breaks. But we can also adopt the design stance to other objects that are not designed by us, e.g. evolution through natural selection, arguably, imbues to a type of design, purpose, or function, to parts of animals; it can be insightful to understand animals in this way. A giraffe's long neck is _for_ something.

Finally, Dennett discusses the intentional stance. Intentionality is the capacity of an object to be _about_ something. Mental states have this property -- they are about something. E.g we have beliefs, desires, thoughts about things in the world. To adopt the intentional stance is to assume that an object has intentionality. We, of course, treat other people this way. But we might also adopt the intentional stance towards inanimate objects. For example, we might casually think of a thermostat as 'wanting' to keep the temperature within a certain range. In this case it's rather obvious that the thermostat is significantly more robustly understood from either the physical or design stance. But in general, what determines when something is most robustly understood from the intentional stance? What is a 'true believer', to use Dennett's phrase? I won't explore this question in detail, though it is an interesting one.

So what does this have to do with AI? Well, a programmer or researcher in AI deals adopts a physical stance, thinking of the precise mechanisms that determine an object's behavior. In this case, a clear understanding of such principles can preclude the need to adopt the intentional stance. Those who are not programmers or AI researchers may instead take for granted that those scientists have a mechanistic explanation of an object, say, robot, and thus may also be less inclined to adopt the intentional stance. Of course, this is dependent somewhat on how the research is presented to the public. It's easy for PR departments of AI companies and the press to Research shows that how people interact with a robot is affected by simple and quite superficial changes in a robot's behavior or appearance, such as the addition of eyes or facial expressions. 

This answer 




